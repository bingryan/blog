<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>BGD、SGD和MBGD的一些区别 - 人生总得写点什么</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="人生总得写点什么"><meta name="msapplication-TileImage" content="/images/avatar.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="人生总得写点什么"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本文主要讲解了 梯度下降(Batch gradient descent)，随机梯度下降(Stochastic gradient descent)，小批量梯度下降(Mini-batch gradient descent)在实现上的区别"><meta property="og:type" content="blog"><meta property="og:title" content="BGD、SGD和MBGD的一些区别"><meta property="og:url" content="http://www.bingryan.com/2017/09/12/BGD%E3%80%81SGD%E5%92%8CMBGD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8C%BA%E5%88%AB/"><meta property="og:site_name" content="人生总得写点什么"><meta property="og:description" content="本文主要讲解了 梯度下降(Batch gradient descent)，随机梯度下降(Stochastic gradient descent)，小批量梯度下降(Mini-batch gradient descent)在实现上的区别"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://www.bingryan.com/img/og_image.png"><meta property="article:published_time" content="2017-09-11T19:08:56.000Z"><meta property="article:modified_time" content="2020-11-25T08:16:34.900Z"><meta property="article:author" content="Ryan"><meta property="article:tag" content="机器学习"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.bingryan.com/2017/09/12/BGD%E3%80%81SGD%E5%92%8CMBGD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8C%BA%E5%88%AB/"},"headline":"BGD、SGD和MBGD的一些区别","image":["http://www.bingryan.com/img/og_image.png"],"datePublished":"2017-09-11T19:08:56.000Z","dateModified":"2020-11-25T08:16:34.900Z","author":{"@type":"Person","name":"Ryan"},"publisher":{"@type":"Organization","name":"人生总得写点什么","logo":{"@type":"ImageObject","url":"http://www.bingryan.com/images/avatar.png"}},"description":"本文主要讲解了 梯度下降(Batch gradient descent)，随机梯度下降(Stochastic gradient descent)，小批量梯度下降(Mini-batch gradient descent)在实现上的区别"}</script><link rel="canonical" href="http://www.bingryan.com/2017/09/12/BGD%E3%80%81SGD%E5%92%8CMBGD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8C%BA%E5%88%AB/"><link rel="icon" href="/images/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/tomorrow-night-eighties.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="人生总得写点什么" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/avatar.png" alt="人生总得写点什么" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/bingryan"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2017-09-11T19:08:56.000Z" title="9/12/2017, 3:08:56 AM">2017-09-12</time>发表</span><span class="level-item"><time dateTime="2020-11-25T08:16:34.900Z" title="11/25/2020, 4:16:34 PM">2020-11-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">11 分钟读完 (大约1684个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">BGD、SGD和MBGD的一些区别</h1><div class="content"><p>本文主要讲解了 梯度下降(Batch gradient descent)，随机梯度下降(Stochastic gradient descent)，小批量梯度下降(Mini-batch gradient descent)在实现上的区别</p>
<a id="more"></a>


<h3 id="梯度下降-Batch-gradient-descent-–BGD"><a href="#梯度下降-Batch-gradient-descent-–BGD" class="headerlink" title="梯度下降(Batch gradient descent)–BGD"></a>梯度下降(Batch gradient descent)–BGD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降(Batch gradient descent)--BGD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x:  输入的x</span></span><br><span class="line"><span class="string">    :param y:  输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="comment"># 全部的值带入，计算 梯度</span></span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>





<h3 id="随机梯度下降-Stochastic-gradient-descent-–SGD"><a href="#随机梯度下降-Stochastic-gradient-descent-–SGD" class="headerlink" title="随机梯度下降(Stochastic gradient descent)–SGD"></a>随机梯度下降(Stochastic gradient descent)–SGD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_false</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> batch_gradient_descent(x, y, learn_rate, epoches)</span><br><span class="line"><span class="comment"># 正确的随机梯度应该是这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_true</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        shufflle_data = np.column_stack((y, x))</span><br><span class="line">        np.random.shuffle(shufflle_data)</span><br><span class="line">        stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">        <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">        y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">        x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 随机之后的值，进行梯度计算</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>



<h3 id="小批量梯度下降-Mini-batch-gradient-descent-–MBGD"><a href="#小批量梯度下降-Mini-batch-gradient-descent-–MBGD" class="headerlink" title="小批量梯度下降(Mini-batch gradient descent)–MBGD"></a>小批量梯度下降(Mini-batch gradient descent)–MBGD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mini_batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches, mini_length</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :param mini_length: mini batch length</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 随机打乱----optional</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="comment"># 随机打乱数据  ----optional</span></span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        <span class="comment"># 0-min_length， mini_length+1  2mini_length, ....... 一小段，一小段距离用于一次优化迭代</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(y), mini_length):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / mini_length</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / mini_length</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>


<h3 id="实验代码"><a href="#实验代码" class="headerlink" title="实验代码"></a>实验代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Date    : 2017/9/8</span></span><br><span class="line"><span class="comment"># @Author  : ryanbing (legotime@qq.com)</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">x = <span class="number">10</span> * rng.rand(<span class="number">500</span>)</span><br><span class="line">y = <span class="number">3</span> * x + <span class="number">2</span> + rng.randn(<span class="number">500</span>)</span><br><span class="line"><span class="comment"># plt.scatter(x, y)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># 找出 y = wx + b 中的w 和 b, 正确的应该是 w = 3, b = 2</span></span><br><span class="line"><span class="comment"># 我们在计算的时候其看成 y = WX 其中 W= [w, b], X = [x, 1].T</span></span><br><span class="line"><span class="comment"># 梯度下降(Batch gradient descent)--BGD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x:  输入的x</span></span><br><span class="line"><span class="string">    :param y:  输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="comment"># 全部的值带入，计算 梯度</span></span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, theta</span><br><span class="line"><span class="comment"># 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_false</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, batch_gradient_descent(x, y, learn_rate, epoches)</span><br><span class="line"><span class="comment"># 正确的随机梯度应该是这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_true</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        shufflle_data = np.column_stack((y, x))</span><br><span class="line">        np.random.shuffle(shufflle_data)</span><br><span class="line">        stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">        <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">        y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">        x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 随机之后的值，进行梯度计算</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, theta</span><br><span class="line"><span class="comment"># 小批量梯度下降(Mini-batch gradient descent)--MBGD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mini_batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches, mini_length</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :param mini_length: mini batch length</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    <span class="comment"># 随机打乱----optional</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="comment"># 随机打乱数据  ----optional</span></span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        <span class="comment"># 0-min_length， mini_length+1  2mini_length, ....... 一小段，一小段距离用于一次优化迭代</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(y), mini_length):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / mini_length</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / mini_length</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, theta</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">contro_func</span>(<span class="params">func, **kwargs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param func: 函数</span></span><br><span class="line"><span class="string">    :param kwargs:  func 中需要的参数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = kwargs.get(<span class="string">&#x27;x&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    y = kwargs.get(<span class="string">&#x27;y&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    learn_rate = kwargs.get(<span class="string">&#x27;learn_rate&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    epoches = kwargs.get(<span class="string">&#x27;epoches&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    stochastic_rate = kwargs.get(<span class="string">&#x27;stochastic_rate&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    mini_length = kwargs.get(<span class="string">&#x27;mini_length&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="comment"># change the value is args is not num</span></span><br><span class="line">    <span class="keyword">if</span> stochastic_rate <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> func(x, y, learn_rate, epoches, stochastic_rate)</span><br><span class="line">    <span class="keyword">if</span> mini_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> func(x, y, learn_rate, epoches, mini_length)</span><br><span class="line">    <span class="keyword">return</span> func(x, y, learn_rate, epoches)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trend</span>():</span></span><br><span class="line">    <span class="comment"># 画出收敛的的图像和收敛对应的时间</span></span><br><span class="line">    rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">    x = <span class="number">10</span> * rng.rand(<span class="number">500</span>)</span><br><span class="line">    x = np.array([x, np.ones(<span class="number">500</span>)]).T</span><br><span class="line">    y = <span class="number">3</span> * x + <span class="number">2</span> + rng.randn(<span class="number">500</span>)</span><br><span class="line">    learn_rate = <span class="number">0.01</span></span><br><span class="line">    stochastic_rate = <span class="number">0.4</span></span><br><span class="line">    mini_length = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> [batch_gradient_descent, stochastic_gradient_descent_false,</span><br><span class="line">              stochastic_gradient_descent_true, mini_batch_gradient_descent]:</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="keyword">for</span> epoches <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>, <span class="number">10000</span>, <span class="number">100000</span>]:</span><br><span class="line">            tmp.append(contro_func(i, x=x, y=y, learn_rate=learn_rate, stochastic_rate=stochastic_rate,</span><br><span class="line">                                   mini_length=mini_length, epoches=epoches))</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># test(func=func, x=1, y=2, learn_rate=3, epoches=4, stochastic_rate=5)</span></span><br><span class="line">    <span class="comment"># print(batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100000))</span></span><br><span class="line">    <span class="comment"># [ 1.14378512 0.17288215]</span></span><br><span class="line">    <span class="comment"># [ 3.18801281 0.50870366]</span></span><br><span class="line">    <span class="comment"># [ 3.18602557 0.806018 ]</span></span><br><span class="line">    <span class="comment"># [ 3.03276102 1.84267445]</span></span><br><span class="line">    <span class="comment"># [ 3.01449298 1.96623647]</span></span><br><span class="line">    <span class="comment"># [ 3.01449298 1.96623647]</span></span><br><span class="line">    <span class="comment"># print(stochastic_gradient_descent_false(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100,stochastic_rate=0.4))</span></span><br><span class="line">    <span class="comment"># [ 1.11939055 0.16949282]</span></span><br><span class="line">    <span class="comment"># [ 3.19877639 0.50404936]</span></span><br><span class="line">    <span class="comment"># [ 3.20921332 0.78698163]</span></span><br><span class="line">    <span class="comment"># [ 3.04720128 1.82412805]</span></span><br><span class="line">    <span class="comment"># [ 3.01920995 1.89883629]</span></span><br><span class="line">    <span class="comment"># [ 2.98281143 2.15226071]</span></span><br><span class="line">    <span class="comment"># print(stochastic_gradient_descent_true(np.array([x, np.ones(50000)]).T, y, learn_rate=0.01, epoches=1000,stochastic_rate=1))</span></span><br><span class="line">    <span class="comment"># print(mini_batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100, mini_length=10))</span></span><br><span class="line">    <span class="comment"># [ 0.94630842  0.14845568]</span></span><br><span class="line">    <span class="comment"># [ 0.8811451   0.15444328]</span></span><br><span class="line">    <span class="comment"># [ 3.18337012  0.51049921]</span></span><br><span class="line">    <span class="comment"># [ 3.14833317  0.79174635]</span></span><br><span class="line">    <span class="comment"># [ 3.03507147  1.87931184]</span></span><br></pre></td></tr></table></figure>


</div><div class="article-licensing box"><div class="licensing-title"><p>BGD、SGD和MBGD的一些区别</p><p><a href="http://www.bingryan.com/2017/09/12/BGD、SGD和MBGD的一些区别/">http://www.bingryan.com/2017/09/12/BGD、SGD和MBGD的一些区别/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Ryan</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2017-09-12</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-11-25</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/08/27/%E3%80%8AHOW-TO-DEFI%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">《HOW-TO-DEFI》读书笔记</span></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "3140b5db4ef762fc3527bba7e7a83b8d",
            repo: "https://github.com/bingryan/bingryan.github.io.git",
            owner: "bingryan",
            clientID: "89826d23899fa6f50875",
            clientSecret: "c7eef5b94f061c690213e8e59ca5db4879e687fb",
            admin: ["bingryan"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="http://github.com/bingryan" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">GitHub</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Mac/"><span class="level-start"><span class="level-item">Mac</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%96%B9%E6%B3%95/"><span class="level-start"><span class="level-item">方法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">读书笔记</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-25T08:56:44.000Z">2021-07-25</time></p><p class="title"><a href="/2021/07/25/%E4%B8%AA%E4%BA%BAMac%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">个人Mac环境配置</a></p><p class="categories"><a href="/categories/Mac/">Mac</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-02T08:11:35.000Z">2020-12-02</time></p><p class="title"><a href="/2020/12/02/%E3%80%8A%E8%BF%90%E5%8A%A8%E6%94%B9%E9%80%A0%E5%A4%A7%E8%84%91%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《运动改造大脑》读书笔记</a></p><p class="categories"><a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-11-27T03:01:40.000Z">2020-11-27</time></p><p class="title"><a href="/2020/11/27/%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87-%E5%90%B4%E6%81%A9%E8%BE%BE/">如何有效阅读论文-吴恩达</a></p><p class="categories"><a href="/categories/%E6%96%B9%E6%B3%95/">方法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-11-25T08:06:17.000Z">2020-11-25</time></p><p class="title"><a href="/2020/11/25/%E3%80%8A%E7%BB%88%E7%BB%93%E6%8B%96%E5%BB%B6%E7%97%87%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《终结拖延症》读书笔记</a></p><p class="categories"><a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-10-13T03:47:57.000Z">2020-10-13</time></p><p class="title"><a href="/2020/10/13/%E3%80%8A%E5%AD%A6%E4%BC%9A%E6%8F%90%E9%97%AE%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《学会提问》读书笔记</a></p><p class="categories"><a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.png" alt="Ryan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Ryan</p><p class="is-size-6 is-block">人生总得写点什么</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>CN</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/bingryan" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/bingryan"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="CSDN" href="https://blog.csdn.net/legotime"><i class="crosshairs"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#梯度下降-Batch-gradient-descent-–BGD"><span class="level-left"><span class="level-item">1</span><span class="level-item">梯度下降(Batch gradient descent)–BGD</span></span></a></li><li><a class="level is-mobile" href="#随机梯度下降-Stochastic-gradient-descent-–SGD"><span class="level-left"><span class="level-item">2</span><span class="level-item">随机梯度下降(Stochastic gradient descent)–SGD</span></span></a></li><li><a class="level is-mobile" href="#小批量梯度下降-Mini-batch-gradient-descent-–MBGD"><span class="level-left"><span class="level-item">3</span><span class="level-item">小批量梯度下降(Mini-batch gradient descent)–MBGD</span></span></a></li><li><a class="level is-mobile" href="#实验代码"><span class="level-left"><span class="level-item">4</span><span class="level-item">实验代码</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">七月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">十月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/09/"><span class="level-start"><span class="level-item">九月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Mac/"><span class="tag">Mac</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%B9%E6%B3%95/"><span class="tag">方法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"><span class="tag">读书笔记</span><span class="tag">5</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/avatar.png" alt="人生总得写点什么" height="28"></a><p class="is-size-7"><span>&copy; 2021 Ryan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>